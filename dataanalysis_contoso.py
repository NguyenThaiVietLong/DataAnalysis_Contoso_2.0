import pandas as pd
import streamlit as st
from sklearn.ensemble import RandomForestRegressor
from package.dataStatistics import *
from package.preProcessing import *
from package.dataIntergration import *
from package.machineLearning import *
from package.visualizing import *
from st_pages import Page, Section, show_pages, add_page_title, hide_pages


add_page_title()

@st.cache_data
def load_data():
    # data1 = pd.read_excel('data/2008 Contoso Data.xlsx')
    # data2 = pd.read_excel('data/2009 Contoso Data.xlsx')
    data1 = pd.read_json("data/jsonl/2008_Contoso_Data.jsonl", lines=True)
    data2 = pd.read_json("data/jsonl/2009_Contoso_Data.jsonl", lines=True)
    data = pd.concat([data1, data2], ignore_index=True)

    data3 = pd.read_excel('data/excel/Contoso_Lookup_Tables.xlsx', sheet_name=None)
    # data = pd.concat([data1, data2], ignore_index=True)

    product_data = pd.read_json("data/jsonl/DIM_product.jsonl", lines=True)
    # product_data = pd.read_excel('data/Contoso Lookup Tables.xlsx', sheet_name='DIM Product')
    # product_subcategory_data = pd.read_excel('data/Contoso Lookup Tables.xlsx', sheet_name='DIM Product Sub Category')
    product_subcategory_data = pd.read_json("data/jsonl/DIM_product_subcategory_data.jsonl", lines=True)
    # geography_data = pd.read_excel('data/Contoso Lookup Tables.xlsx', sheet_name='DIM Geography')
    geography_data = pd.read_json("data/jsonl/DIM_geography_data.jsonl", lines=True)
    # channel_data =  pd.read_excel('data/Contoso Lookup Tables.xlsx', sheet_name='DIM Channel')
    channel_data =  pd.read_json("data/jsonl/DIM_channel_data.jsonl", lines=True)
    return data1,data2, data3, data, product_data, product_subcategory_data, geography_data, channel_data

if 'data_loaded' not in st.session_state:
    (st.session_state['data1'], st.session_state['data2'],
     st.session_state['data3'], st.session_state['data'],
     st.session_state['product_data'], st.session_state['product_subcategory_data'],
     st.session_state['geography_data'], st.session_state['channel_data']) = load_data()
    
    st.session_state['data_loaded'] = True
# @st.cache_data
# def load_model():
#     new_data = dataReduction(data)
#     x = new_data.drop('SalesAmount',axis = 1)
#     y = new_data['SalesAmount']
#     x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42) 
#     # model_rf = RandomForestRegressor().fit(x_train, y_train)
#     model_lr = LinearRegression().fit(x_train, y_train)
#     model_gr = GradientBoostingRegressor().fit(x_train, y_train)
#     return model_gr, model_lr, new_data
# # Load data and random_forest model only once
# data1, data2,data3, data, product_data, product_subcategory_data, geography_data, channel_data = load_data()
# model_gr, model_lr, new_data = load_model()

# model_rf = LinearRegression().fit(x_train, y_train)


# H√†m t·∫°o li√™n k·∫øt ƒë·∫øn c√°c tab


# T·∫°o c√°c tab trong sidebar
# tabs = ['Tab 1', 'Tab 2', 'Tab 3','Tab 4', 'Tab 5', 'Tab 6']
# selected_page = st.selectbox('Ch·ªçn trang:', ['Trang 1', 'Trang 2', 'Trang 3','Trang 4', 'Trang 5', 'Trang 6','Trang 7', 'Trang 8', 'Trang 9','Trang 10', 'Trang 11', 'Trang 12','Trang 13','Trang 14','Trang 15','Trang 16','Trang 17', 'Trang 18'])
# pages = [
#     'Trang 1', 'Trang 2', 'Trang 3', 'Trang 4', 'Trang 5', 
#     'Trang 6', 'Trang 7', 'Trang 8', 'Trang 9', 'Trang 10', 
#     'Trang 11', 'Trang 12', 'Trang 13', 'Trang 14', 'Trang 15', 
#     'Trang 16', 'Trang 17', 'Trang 18'
# ]
# selected_page = st.sidebar.selectbox('Ch·ªçn trang:', pages)

# N·ªôi dung c·ªßa tab ƒë∆∞·ª£c ch·ªçn
show_pages(
    [
        Page("dataanalysis_contoso.py", "Data Analysis - Contoso", "üíª"),
        # Page("main1.py", "Describe Dataset", "üíª"),
        # Page("main2.py", "Data Statistics ", "üíª"),
        # Page("main3.py", "Data Preprocessing ", "üíª"),
        Section( "Data Visualization ", "üíª"),
        Page("DataVisualization/ProfitContoso.py", "Profit Contoso", "üíª", in_section=True),
        Page("DataVisualization/wdr.py", "Why Decrease Revenue ", "üíª", in_section=True),
        Page("DataVisualization/supProfit.py", "supProfit ", "üíª", in_section=True),
        # Page("main.py", "Quantity Sale by Class ", "üíª", in_section=True),
        # Page("main.py", "Revenue by Class ", "üíª", in_section=True),
        # Page("main.py", "Quantity Sold by Brand ", "üíª", in_section=True),
        # Page("main.py", "Quantity Sales by Product Subcategory ", "üíª", in_section=True),
        # Page("main.py", "Number of Stores by Region ", "üíª", in_section=True),
        # Page("main.py", "Profit by Region ", "üíª", in_section=True),
        # Page("main.py", "Revenues by Channel ", "üíª", in_section=True),
        # Page("main.py", "Summary and Propose ", "üíª", in_section=True),
        # Section( "Data Modeling ", "üíª"),
        # Page("main.py", "Heatmap ", "üíª", in_section=True),
        # Page("main.py", "Data Collection ", "üíª", in_section=True),
        # Section( "Machine Learning ", "üíª"),
        # Page("main.py", "Calculate model points ", "üíª", in_section=True),
        # Page("main.py", "Test model against dataset ", "üíª", in_section=True),
        # Page("main.py", "Demo model ", "üíª", in_section=True),

    ]
)
hide_pages(["Thank you"])

st.markdown("### üë®‚Äçüîß Data Analyst by Vi·ªát Long & S∆°n L·ªôc")

st.image("https://statics.cdn.200lab.io/2021/07/59-data-analysis-1.jpg")

st.info("Original Course Repository on [Github](https://github.com/DataTalksClub/data-engineering-zoomcamp)")

st.markdown("---")

with st.expander("Sign up here for 2024 Cohort"):
    st.markdown("""
    
    <a href="https://airtable.com/appzbS8Pkg9PL254a/shr6oVXeQvSI5HuWD"><img src="https://user-images.githubusercontent.com/875246/185755203-17945fd1-6b64-46f2-8377-1011dcb1a444.png" height="50" /></a>

    #

    - Register in [DataTalks.Club's Slack](https://datatalks.club/slack.html)
    - Join the [`#course-data-engineering`](https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG) channel
    - Join the [course Telegram channel with announcements](https://t.me/dezoomcamp)
    - The videos are published on [DataTalks.Club's YouTube channel](https://www.youtube.com/c/DataTalksClub) in [the course playlist](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)
    - [Frequently asked technical questions](https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing)
        
    #""", unsafe_allow_html=True)

st.markdown("""
### üë®‚Äçüéì Taking the course

##### üë®‚Äçüë¶‚Äçüë¶ 2024 Cohort

* **Start**: 15 January 2024 (Monday) at 17:00 CET
* **Registration link**: https://airtable.com/shr6oVXeQvSI5HuWD
* [Cohort folder](cohorts/2024/) with homeworks and deadlines 


##### üë®‚Äçüîß Self-paced mode

All the materials of the course are freely available, so that you
can take the course at your own pace

* Follow the suggested syllabus (see below) week by week
* You don't need to fill in the registration form. Just start watching the videos and join Slack
* Check [FAQ](https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing) if you have problems
* If you can't find a solution to your problem in FAQ, ask for help in Slack

### üîé Overview""", unsafe_allow_html=True)


st.image("https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/images/architecture/photo1700757552.jpeg")

# if selected_page  == 'Trang 1':

#     st.markdown('<h1 ">M·ª•c L·ª•c</h1>', unsafe_allow_html=True)
#     st.header('1. M√î T·∫¢ V·ªÄ B·ªò D·ªÆ LI·ªÜU',anchor = 'Trang 2', )
#     st.header('2.TH·ªêNG K√ä D·ªÆ LI·ªÜU',)
#     st.header('3. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU',)
#     st.header('4. TR·ª∞C QUAN H√ìA D·ªÆ LI·ªÜU',)
#     st.header('5. M√î H√åNH H√ìA D·ªÆ LI·ªÜU',)
#     st.header('6. K·∫æT LU·∫¨N',)


# elif selected_page  == 'Trang 2':
#     st.header('M√î T·∫¢ V·ªÄ B·ªò D·ªÆ LI·ªÜU',divider = 'rainbow')
#     col1, col2 = st.columns(2)
#     col1.header('1. Company Background',)
#     col1.markdown("""
# <ul>
#   <li>Contoso l√† m·ªôt c√¥ng ty b√°n l·∫ª ƒëa qu·ªëc gia</li>
#   <li>Th·ªùi gian: 2008 - 2009</li>
#   <li>Th·ªã tr∆∞·ªùng: North America, Australia, Asia, Europe</li>
#   <li>K√™nh ph√¢n ph·ªëi: Online, Store, Catalog, Reseller</li>
#   <li>S·∫£n ph·∫©m: Thi·∫øt b·ªã ƒëi·ªán t·ª≠ v√† d·ªãch v·ª• media</li>
# </ul>
#  """, unsafe_allow_html=True)
#     col2.image('./Photo/2.jfif', caption='C√¥ng ty Contoso', width=350)

#     st.header('2. M√¥ h√¨nh Star Schema')
#     st.write('- Data Star Schema l√† m·ªôt m√¥ h√¨nh t·ªï ch·ª©c d·ªØ li·ªáu, n∆°i c√≥ m·ªôt b·∫£ng trung t√¢m (fact table) ƒë·∫°i di·ªán cho s·ª± ki·ªán ho·∫∑c th√¥ng tin c∆° b·∫£n, ƒë∆∞·ª£c bao quanh b·ªüi c√°c b·∫£ng chi·ªÅu (dimension tables) m√¥ t·∫£ chi ti·∫øt.\n\n- ƒêi·ªÉm m·∫°nh c·ªßa m√¥ h√¨nh n√†y n·∫±m ·ªü kh·∫£ nƒÉng t·ªëi ∆∞u hi·ªáu su·∫•t truy v·∫•n v√† ph√¢n t√≠ch d·ªØ li·ªáu.\n\n- C√°c b·∫£ng chi·ªÅu ƒë∆∞·ª£c t·ªï ch·ª©c linh ho·∫°t v√† d·ªÖ hi·ªÉu, gi√∫p gi·∫£m th·ªùi gian x·ª≠ l√Ω, tƒÉng c∆∞·ªùng kh·∫£ nƒÉng hi·ªÉn th·ªã th√¥ng tin, v√† h·ªó tr·ª£ m·ªü r·ªông d·ªØ li·ªáu m·ªôt c√°ch thu·∫≠n ti·ªán.')   
#     st.image('./Photo/1.png', caption='M√¥ h√¨nh Star Schema')
#     st.write(data)
#     tabs = st.tabs([sheet_name for sheet_name in data3.keys()])
#     for index, sheet_name in enumerate(data3.keys()):
#         with tabs[index]: 
#             st.write(data3[sheet_name])
    
    
# elif selected_page  == 'Trang 3':
#     st.header('TH·ªêNG K√ä D·ªÆ LI·ªÜU',)
#     st.header('DATA 2008')
#     dataStatistics2008(data1)
#     st.header('DATA 2009')
#     dataStatistics2009(data2)


    
    
    

# elif selected_page   == 'Trang 4':
#     handleMissingData2008(data1)    
#     handleDuplicateData2008(data1)
#     checkDataType2008(data1)
#     handleMissingData2009(data2)
#     handleDuplicateData2009(data2)
#     checkDataType2009(data2)

#     dI_preProcessing(data)
    
# elif selected_page == 'Trang 5':
    
#     st.subheader('L·ª¢I NHU·∫¨N NƒÇM 2008 - 2009')
#     revenueOverTime(data1, data2)

# elif selected_page == 'Trang 6':
#     st.header('B√ÄI TO√ÅN: T·∫†I SAO L·ª¢I NHU·∫¨N GI·∫¢M')
#     col1, col2 = st.columns(2)
#     col1.image('./Photo/1496.jpg', caption='T√¨nh h√¨nh kinh t·∫ø n∆∞·ªõc M·ªπ c√°c nƒÉm tr∆∞·ªõc 2010')
#     col2.image('./Photo/4.jpg', caption='T·ª∑ l·ªá th·∫•t nghi·ªáp ·ªü M·ªπ nƒÉm 2009')

# elif selected_page == 'Trang 7':
    
#     st.header('TR·ª∞C QUAN H√ìA D·ªÆ LI·ªÜU',)
#     st.subheader('1. DOANH THU C·ª¶A NH√Ä CUNG C·∫§P')
#     revenuebyManufacturer(data,product_data)
    
    
# elif selected_page == 'Trang 8':
#     quantitySalesByClass(data, product_data)
# elif selected_page == 'Trang 9':
#     st.subheader('4. S·ªê L∆Ø·ª¢NG M·∫∂T H√ÄNG THEO TH∆Ø∆†NG HI·ªÜU')
#     quantitySoldByBrand(data, product_data)
# elif selected_page == 'Trang 10':
#     st.subheader('5. S·ªê L∆Ø·ª¢NG M·∫∂T H√ÄNG THEO LO·∫†I S·∫¢N PH·∫®M')
#     quantitySalesByProductSub(product_data, product_subcategory_data,data)
# elif selected_page == 'Trang 11':
#     st.subheader('5. S·ªê L∆Ø·ª¢NG C·ª¨A H√ÄNG THEO KHU V·ª∞C')
#     storeCount(geography_data)
#     st.subheader('6. L·ª¢I NHU·∫¨N THEO KHU V·ª∞C')
#     profitByQuarterForAll(data, geography_data)
# elif selected_page == 'Trang 12':
#     st.subheader('7. DOANH THU THEO K√äNH B√ÅN H√ÄNG')
#     revenueByChannelPieChart(data, channel_data)
#     revenueByYearForCatalog(data, channel_data)

# elif selected_page == 'Trang 13':
#     st.header('ƒê·ªÄ XU·∫§T',)
#     st.subheader('- T·∫≠p trung c√°c s·∫£n ph·∫©m ti·ªán l·ª£i, d·ªÖ s·ª≠ d·ª•ng ')
#     st.subheader('- M·ªü r·ªông th·ªã tr∆∞·ªùng Ch√¢u √Å ')
#     st.subheader('- Ph√°t tri·ªÉn k√™nh b√°n h√†ng tr·ª±c tuy·∫øn ')


# elif selected_page == 'Trang 14':
#     st.header('M√î H√åNH H√ìA D·ªÆ LI·ªÜU',)
#     heatMap(data)
    



# elif selected_page == 'Trang 15':
#     st.header('THU GI·∫¢M D·ªÆ LI·ªÜU',)
#     data = dataReduction(data)
#     st.write(data)
# elif selected_page == 'Trang 16':
#     st.header('M√î H√åNH H√ìA D·ªÆ LI·ªÜU',)
#     data = dataReduction(data)
#     machineLearning(data)


# elif selected_page == 'Trang 17':
#     st.header('KI·ªÇM TRA B·ªò D·ªÆ LI·ªÜU',)
#     predictValue(data=new_data, model_gr = model_gr, model_lr = model_lr)

# elif selected_page == 'Trang 18':
#     st.header('DEMO',)
#     predictValue2(model_gr)










# T√≠nh ma tr·∫≠n t∆∞∆°ng quan
#    
